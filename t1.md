# Старт открытого курса OpenDataScience

Привет всем, кто ждал запуска открытого курса по практическому анализу данных и машинному обучению! 

<img align="center" src="https://habrastorage.org/files/10c/15f/f3d/10c15ff3dcb14abdbabdac53fed6d825.jpg"/>
<br>

Первая статья посвящена первичному анализу данных с Pandas. 

Пока в серии планируется 7 статей, идущих вместе с тетрадками Jupyter ([репозиторий](https://github.com/Yorko/mlcourse_open) mlcourse_open), соревнованиями и домашними заданиями. 

Далее идет список будущих статей, описание курса и, собственно, первая тема – введение в Pandas.

<cut />


### План серии статей
1. [Первичный анализ данных с Pandas](https://habrahabr.ru/company/ods/blog/322626/)
2. [Визуальный анализ данных c Python](https://habrahabr.ru/company/ods/blog/323210/)
3. [Классификация, деревья решений и метод ближайших соседей](https://habrahabr.ru/company/ods/blog/322534/)
4. [Линейные модели классификации и регрессии](https://habrahabr.ru/company/ods/blog/323890/)
5. [Композиции: бэггинг, случайный лес](https://habrahabr.ru/company/ods/blog/324402/)
6. [Построение и отбор признаков. Приложения в задачах обработки текста, изображений и геоданных](https://habrahabr.ru/company/ods/blog/325422/)
7. Обучение без учителя: PCA, кластеризация, поиск аномалий

### План этой статьи
 1. О курсе
 2. Домашние задания в курсе
 3. Демонстрация основных методов Pandas
 4. Первые попытки прогнозирования оттока
 5. Домашнее задание №1
 6. Обзор полезных ресурсов

## 1. О курсе
Многие в нашей команде OpenDataScience занимаются state-of-the-art технологиями машинного обучения: DL-фреймворками, байесовскими методами машинного обучения, вероятностным программированием и не только. По этим темам мы готовим лекции и практические занятия, а для того, чтобы подготовить слушателей к ним, решили опубликовать серию вводных статей на Хабре. 

Мы не ставим себе задачу разработать еще один *исчерпывающий* вводный курс по машинному обучению или анализу данных (т.е. это не замена специализации Яндекса и МФТИ, дополнительному образованию ВШЭ и прочим фундаментальным онлайн- и оффлайн-программам и книжкам). Цель этой серии статей — быстро освежить имеющиеся у вас знания или помочь найти темы для дальнейшего изучения. Подход примерно как у авторов книги *Deep Learning*, которая начинается с обзора математики и основ машинного обучения — краткого, максимально ёмкого и с обилием ссылок на источники. 

Если вы планируете пройти курс, то сразу предупреждаем: при подборе тем и создании материалов мы ориентируемся на то, что наши слушатели знают **математику на уровне 2 курса** технического вуза и хотя бы немного умеют **программировать на Python**. Это не жёсткие критерии отбора, а всего лишь рекомендации — можно записаться на курс, не зная математики или Python (или даже обоих!), и параллельно навёрстывать:

-  базовую математику (мат. анализ, линейную алгебру, оптимизацию, теорвер и статистику) можно повторить по [этим](https://yadi.sk/d/yEXkABC_353Zmh) конспектам Yandex & MIPT (делимся с разрешения). Кратко, на русском – то что надо. Если подробно, то матан – Кудрявцев, линал – Кострикин, оптимизация – Boyd (англ.), теорвер и статистика – Кибзун. Плюс отличные онлайн-курсы МФТИ и ВШЭ на Coursera;
-  по Python хватит небольшого интерактивного туториала на Datacamp или [этого репозитория](https://github.com/yorko/python_intro) по Python и базовым алгоритмам и структурам данных. Что-то более продвинутое – это, например, курс питерского Computer Science Center;
-  что касается машинного обучения, то в 95% случаев вам в первую очередь посоветуют классический курс от Andrew Ng (Stanford, Coursera). На русском языке есть отличная специализация МФТИ и Яндекса «Машинное обучение и анализ данных». А также *must-read*: "The elements of statistical learning" (Hastie, Tibshirani), "Pattern recognition" (Bishop), "Machine Learning: A Probabilistic Perspective " (Murphy) и "Deep Learning" (Goodfellow, Bengio). Книга Bengio и Goodfellow начинается с понятного и интересного интро в машинное обучение и внутреннее устройство его алгоритмов.

### Какое ПО нужно
Пока для воспроизведения описанного нужна только сборка [Anaconda](https://www.continuum.io/downloads) с Python 3.6. Чуть позже понадобятся и другие библиотеки, об этом будет сказано дополнительно. 

**Update:** 

Надеемся, что большинство из вас уже знают про PyPI, могут установить Python 3.6 или даже слышали и пользуются [venv](https://docs.python.org/3/library/venv.html) / [virtualenvwrapper](https://virtualenvwrapper.readthedocs.io/en/latest/index.html), и если вы из их числа, пользуйтесь тем, что удобно именно вам (но полную совместимость материалов курса со всеми версиями библиотек мы *не гарантируем*!). 

Для всех остальных  — установите Anaconda: [Quick Install Guide](https://conda.io/docs/install/quick.html). Сборка включает в себя 100 Python-библиотек и поддерживает [ещё 720](https://docs.continuum.io/anaconda/pkg-docs). Есть также облегчённая версия — [Miniconda](https://conda.io/miniconda.html), с ней все нужные библиотеки придётся устанавливать самостоятельно. 

Также можно воспользоваться Docker-контейнером, в котором все необходимое ПО уже установлено. Подробности – в README [репозитория](https://github.com/Yorko/mlcourse_open).

### Как подключиться к курсу
Подключиться можно в любой момент, но дедлайны по домашним заданиям жесткие. 
Для участия:
- Заполните [опрос](https://docs.google.com/forms/d/e/1FAIpQLSdBk_BnCZbooF2qIUOPK0JbBibVgDWjOE4zQpC9oh_Jv8YcXw/viewform), указав в нем *реальное* ФИО и желательно Google-почту (для ДЗ нужен гугл-аккаунт);
- Можете [вступить](http://ods.ai/) в сообщество OpenDataScience, обсуждение курса ведется в канале *#mlcourse_open*.

## 2. Домашние задания в курсе

- Каждая статья сопровождается домашним заданием в виде тетрадки [Jupyter](http://jupyter.org), в которую надо дописать код, и на основе этого выбрать правильный ответ в форме Google (для этого нужен аккаунт Google);
- Максимум за задание – 10 баллов;
- На каждое задание дается 1 неделя, дедлайн жесткий;
- Решения домашних заданий выкладываются в [репозиторий](https://github.com/Yorko/mlcourse_open) сразу после дедлайна;
- В конце серии статей будет подведен итог (рейтинг участников).

## 3. Демонстрация основных методов Pandas 

**[Pandas](http://pandas.pydata.org)** — это библиотека Python, предоставляющая широкие возможности для анализа данных. Данные, с которыми работают датасаентисты, часто хранятся в форме табличек — например, в форматах .csv, .tsv или .xlsx. С помощью библиотеки Pandas такие  табличные данные очень удобно загружать, обрабатывать и анализировать с помощью SQL-подобных запросов. А в связке с библиотеками Matplotlib и Seaborn Pandas предоставляет широкие возможности визуального анализа табличных данных.

Основными структурами данных в Pandas являются классы **Series** и **DataFrame**. Первый из них представляет собой одномерный индексированный массив данных некоторого фиксированного типа. Второй – это двухмерная структура данных, представляющая собой таблицу, каждый столбец которой содержит данные одного типа. Можно представлять её как словарь объектов типа Series. Структура DataFrame отлично подходит для представления реальных данных: строки соответствуют признаковым описаниям отдельных объектов, а столбцы соответствуют признакам.

```python
# Обеспечим совместимость с Python 2 и 3
# pip install future
from __future__ import (absolute_import, division,
                        print_function, unicode_literals)

# отключим предупреждения Anaconda
import warnings
warnings.simplefilter('ignore')

# импортируем Pandas и Numpy
import pandas as pd
import numpy as np
```

Будем показывать основные методы в деле, анализируя [набор данных](https://bigml.com/user/francisco/gallery/dataset/5163ad540c0b5e5b22000383) по оттоку клиентов телеком-оператора. Прочитаем данные (метод `read_csv`) и посмотрим на первые 5 строк с помощью метода `head()`:

```
df = pd.read_csv('../../data/telecom_churn.csv')
```


```
df.head()
```

<img src="https://habrastorage.org/files/978/022/6a8/9780226a800b4a1da342daaa966b4a0e.png"/>
<br>
<spoiler title="Про вывод датафрейма в тетрадке Jupyter ">
В Jupyter-ноутбуках датафреймы Pandas выводятся в виде вот таких красивых табличек, и `print(df.head())` выглядит хуже.
По умолчанию Pandas выводит всего 20 столбцов и 60 строк, поэтому если ваш датафрейм больше, воспользуйтесь функцией `set_option`:

```
pd.set_option('display.max_columns', 100)
pd.set_option('display.max_rows', 100)
```
</spoiler>

Каждая строка представляет собой одного клиента – это **объект** исследования.  
Столбцы – **признаки** объекта.

<spoiler title="Описание признаков">
|  Название  | Описание | Тип |
|---         |--:       |     |
| **State** | Буквенный код штата | номинальный |
| **Account length** | Как долго клиент обслуживается компанией | количественный |
| **Area code** | Префикс номера телефона | количественный  |
| **International plan** | Международный роуминг  (подключен/не подключен) | бинарный |
| **Voice mail plan** | Голосовая почта (подключена/не подключена) | бинарный |
| **Number vmail messages** | Количество голосовых сообщений | количественный |
| **Total day minutes** |  Общая длительность разговоров днем | количественный |
| **Total day calls** | Общее количество звонков днем | количественный |
| **Total day charge** | Общая сумма оплаты за услуги днем | количественный |
| **Total eve minutes** | Общая длительность разговоров вечером | количественный |
| **Total eve calls** | Общее количество звонков вечером | количественный |
| **Total eve charge** | Общая сумма оплаты за услуги вечером | количественный |
| **Total night minutes** | Общая длительность разговоров ночью | количественный |
| **Total night calls** | Общее количество звонков ночью | количественный |
| **Total night charge** | Общая сумма оплаты за услуги ночью | количественный |
| **Total intl minutes** | Общая длительность международных разговоров | количественный |
| **Total intl calls** | Общее количество международных разговоров | количественный |
| **Total intl charge** | Общая сумма оплаты за международные разговоры | количественный |
| **Customer service calls** | Число обращений в сервисный центр | количественный |
  
Целевая переменная: **Churn** – Признак оттока, бинарный признак (1 – потеря клиента, то есть отток). Потом мы будем строить модели, прогнозирующие этот признак по остальным, поэтому мы и назвали его целевым. 
</spoiler>

**Посмотрим на размер данных, названия признаков и их типы.**


```
print(df.shape)
```

    (3333, 20)


Видим, что в таблице 3333 строки и 20 столбцов. Выведем названия столбцов:


```
print(df.columns)
```

    Index(['State', 'Account length', 'Area code', 'International plan',
           'Voice mail plan', 'Number vmail messages', 'Total day minutes',
           'Total day calls', 'Total day charge', 'Total eve minutes',
           'Total eve calls', 'Total eve charge', 'Total night minutes',
           'Total night calls', 'Total night charge', 'Total intl minutes',
           'Total intl calls', 'Total intl charge', 'Customer service calls',
           'Churn'],
          dtype='object')


Чтобы посмотреть общую информацию по датафрейму и всем признакам, воспользуемся методом **`info`**:


```
print(df.info())
```
<cut />
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 3333 entries, 0 to 3332
    Data columns (total 20 columns):
    State                     3333 non-null object
    Account length            3333 non-null int64
    Area code                 3333 non-null int64
    International plan        3333 non-null object
    Voice mail plan           3333 non-null object
    Number vmail messages     3333 non-null int64
    Total day minutes         3333 non-null float64
    Total day calls           3333 non-null int64
    Total day charge          3333 non-null float64
    Total eve minutes         3333 non-null float64
    Total eve calls           3333 non-null int64
    Total eve charge          3333 non-null float64
    Total night minutes       3333 non-null float64
    Total night calls         3333 non-null int64
    Total night charge        3333 non-null float64
    Total intl minutes        3333 non-null float64
    Total intl calls          3333 non-null int64
    Total intl charge         3333 non-null float64
    Customer service calls    3333 non-null int64
    Churn                     3333 non-null bool
    dtypes: bool(1), float64(8), int64(8), object(3)
    memory usage: 498.1+ KB
    None


`bool`, `int64`, `float64` и `object` — это типы признаков. Видим, что 1 признак — логический (bool), 3 признака имеют тип object и 16 признаков — числовые. Также с помощью метода `info` удобно быстро посмотреть на пропуски в данных, в нашем случае их нет, в каждом столбце по 3333 наблюдения.

**Изменить тип колонки** можно с помощью метода `astype`. Применим этот метод к признаку `Churn` и переведём его в `int64`:


```
df['Churn'] = df['Churn'].astype('int64')
```

Метод **`describe`** показывает основные статистические характеристики данных по каждому числовому признаку (типы `int64` и `float64`): число непропущенных значений, среднее, стандартное отклонение, диапазон, медиану, 0.25 и 0.75 квартили.


```
df.describe()
```

<img src="https://habrastorage.org/files/7d2/ca9/ba8/7d2ca9ba80ed4611b2388b6c6170befb.png"/>
<br>

Чтобы посмотреть статистику по нечисловым признакам, нужно явно указать интересующие нас типы в параметре `include`.


```
df.describe(include=['object', 'bool'])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>State</th>
      <th>Area code</th>
      <th>International plan</th>
      <th>Voice mail plan</th>
      <th>Churn</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3333</td>
      <td>3333</td>
      <td>3333</td>
      <td>3333</td>
      <td>3333</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>51</td>
      <td>3</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>top</th>
      <td>WV</td>
      <td>415</td>
      <td>No</td>
      <td>No</td>
      <td>False</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>106</td>
      <td>1655</td>
      <td>3010</td>
      <td>2411</td>
      <td>2850</td>
    </tr>
  </tbody>
</table>
</div>
Для категориальных (тип `object`) и булевых (тип `bool`) признаков  можно воспользоваться методом **`value_counts`**. Посмотрим на распределение данных по нашей целевой переменной — `Churn`:


```
df['Churn'].value_counts()
```




    0    2850
    1     483
    Name: Churn, dtype: int64



2850 пользователей из 3333 — лояльные, значение переменной `Churn` у них — `0`.

Посмотрим на распределение пользователей по переменной `Area code`. Укажем значение параметра `normalize=True`, чтобы посмотреть не абсолютные частоты, а относительные.


```
df['Area code'].value_counts(normalize=True)
```




    415    0.496550
    510    0.252025
    408    0.251425
    Name: Area code, dtype: float64



### Сортировка

DataFrame можно отсортировать по значению какого-нибудь из признаков. В нашем случае, например, по `Total day charge` (`ascending=False` для сортировки по убыванию):


```
df.sort_values(by='Total day charge', 
        ascending=False).head()
```

<img src="https://habrastorage.org/files/dd2/759/ada/dd2759ada8644838a54d2f355b9fec51.png"/>
<br>

Сортировать можно и по группе столбцов:

```
df.sort_values(by=['Churn', 'Total day charge'],
        ascending=[True, False]).head()
```
<br> *спасибо за замечание про устаревший sort* @makkos
 
<img src="https://habrastorage.org/files/b77/c81/8f5/b77c818f5c2448ddac8dc586985198c1.png"/>
<br>

### Индексация и извлечение данных

DataFrame можно индексировать по-разному. В связи с этим рассмотрим различные способы индексации и извлечения нужных нам данных из датафрейма на примере простых вопросов.

Для извлечения отдельного столбца можно использовать конструкцию вида `DataFrame['Name']`. Воспользуемся этим для ответа на вопрос: **какова доля людей нелояльных пользователей в нашем датафрейме?**


```
df['Churn'].mean()
```




    0.14491449144914492



14,5% — довольно плохой показатель для компании, с таким процентом оттока можно и разориться. 

Очень удобной является **логическая индексация** DataFrame по одному столбцу. Выглядит она следующим образом: `df[P(df['Name'])]`, где `P` - это некоторое логическое условие, проверяемое для каждого элемента столбца `Name`. Итогом такой индексации является DataFrame, состоящий только из строк, удовлетворяющих условию `P` по столбцу `Name`. 

Воспользуемся этим для ответа на вопрос: **каковы средние значения числовых признаков среди нелояльных пользователей?**


```
df[df['Churn'] == 1].mean()
```




    Account length            102.664596
    Number vmail messages       5.115942
    Total day minutes         206.914079
    Total day calls           101.335404
    Total day charge           35.175921
    Total eve minutes         212.410145
    Total eve calls           100.561077
    Total eve charge           18.054969
    Total night minutes       205.231677
    Total night calls         100.399586
    Total night charge          9.235528
    Total intl minutes         10.700000
    Total intl calls            4.163561
    Total intl charge           2.889545
    Customer service calls      2.229814
    Churn                       1.000000
    dtype: float64



Скомбинировав предыдущие два вида индексации, ответим на вопрос: **сколько в среднем в течение дня разговаривают по телефону нелояльные пользователи**?


```
df[df['Churn'] == 1]['Total day minutes'].mean()
```




    206.91407867494823



**Какова максимальная длина международных звонков среди лояльных пользователей (`Churn == 0`), не пользующихся услугой международного роуминга (`'International plan' == 'No'`)?**


```
df[(df['Churn'] == 0) & (df['International plan'] == 'No')]['Total intl minutes'].max()
```




    18.899999999999999



Датафреймы можно индексировать как по названию столбца или строки, так и по порядковому номеру. Для индексации **по названию** используется метод **`loc`**, **по номеру** — **`iloc`**.

В первом случае мы говорим *«передай нам значения первых пяти строк в столбцах от State до Area code»*, а во втором — *«передай нам значения первых пяти строк в первых трёх столбцах»*. 


```
df.loc[0:5, 'State':'Area code']
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>State</th>
      <th>Account length</th>
      <th>Area code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KS</td>
      <td>128</td>
      <td>415</td>
    </tr>
    <tr>
      <th>1</th>
      <td>OH</td>
      <td>107</td>
      <td>415</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NJ</td>
      <td>137</td>
      <td>415</td>
    </tr>
    <tr>
      <th>3</th>
      <td>OH</td>
      <td>84</td>
      <td>408</td>
    </tr>
    <tr>
      <th>4</th>
      <td>OK</td>
      <td>75</td>
      <td>415</td>
    </tr>
    <tr>
      <th>5</th>
      <td>AL</td>
      <td>118</td>
      <td>510</td>
    </tr>
  </tbody>
</table>
</div>

```
df.iloc[0:5, 0:3]
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>State</th>
      <th>Account length</th>
      <th>Area code</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>KS</td>
      <td>128</td>
      <td>415</td>
    </tr>
    <tr>
      <th>1</th>
      <td>OH</td>
      <td>107</td>
      <td>415</td>
    </tr>
    <tr>
      <th>2</th>
      <td>NJ</td>
      <td>137</td>
      <td>415</td>
    </tr>
    <tr>
      <th>3</th>
      <td>OH</td>
      <td>84</td>
      <td>408</td>
    </tr>
    <tr>
      <th>4</th>
      <td>OK</td>
      <td>75</td>
      <td>415</td>
    </tr>
  </tbody>
</table>
</div>
Если нам нужна первая или последняя строчка датафрейма, пользуемся конструкцией `df[:1]` или `df[-1:]`:


```
df[-1:]
```

<img src="https://habrastorage.org/files/be1/171/b2f/be1171b2f036492d8e99c7a05dad8717.png"/>
<br>

### Применение функций к ячейкам, столбцам и строкам

**Применение функции к каждому столбцу: apply**


```
df.apply(np.max) 
```




    State                        WY
    Account length              243
    Area code                   510
    International plan          Yes
    Voice mail plan             Yes
    Number vmail messages        51
    Total day minutes         350.8
    Total day calls             165
    Total day charge          59.64
    Total eve minutes         363.7
    Total eve calls             170
    Total eve charge          30.91
    Total night minutes         395
    Total night calls           175
    Total night charge        17.77
    Total intl minutes           20
    Total intl calls             20
    Total intl charge           5.4
    Customer service calls        9
    Churn                      True
    dtype: object



Метод `apply` можно использовать и для того, чтобы применить функцию к каждой строке. Для этого нужно указать `axis=1`.

**Применение функции к каждой ячейке столбца: `map`**

Например, метод `map` можно использовать для **замены значений в колонке**, передав ему в качестве аргумента словарь вида `{old_value: new_value}`:


```
d = {'No' : False, 'Yes' : True}
df['International plan'] = df['International plan'].map(d)
df.head()
```
<img src="https://habrastorage.org/files/65c/63a/6f6/65c63a6f622342e48764b59f1b00b1d8.png"/>
<br>

Аналогичную операцию можно провернуть с помощью метода `replace`:

```
df = df.replace({'Voice mail plan': d})
df.head()
```

<img src="https://habrastorage.org/files/991/e9b/d4d/991e9bd4d43d45cb91de19138b5b8042.png"/>
<br>

### Группировка данных

В общем случае группировка данных в Pandas выглядит следующим образом:

```
df.groupby(by=grouping_columns)[columns_to_show].function()
```

1. К датафрейму применяется метод **`groupby`**, который разделяет данные по `grouping_columns` – признаку или набору признаков.
3. Выбираем нужные нам столбцы (`columns_to_show`). 
2. К полученным группам применяется функция или несколько функций.

**Группирование данных в зависимости от значения признака `Churn` и вывод статистик по трём столбцам в каждой группе.**


```
columns_to_show = ['Total day minutes', 'Total eve minutes', 'Total night minutes']

df.groupby(['Churn'])[columns_to_show].describe(percentiles=[])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Total day minutes</th>
      <th>Total eve minutes</th>
      <th>Total night minutes</th>
    </tr>
    <tr>
      <th>Churn</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="6" valign="top">0</th>
      <th>count</th>
      <td>2850.000000</td>
      <td>2850.000000</td>
      <td>2850.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>175.175754</td>
      <td>199.043298</td>
      <td>200.133193</td>
    </tr>
    <tr>
      <th>std</th>
      <td>50.181655</td>
      <td>50.292175</td>
      <td>51.105032</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>23.200000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>177.200000</td>
      <td>199.600000</td>
      <td>200.250000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>315.600000</td>
      <td>361.800000</td>
      <td>395.000000</td>
    </tr>
    <tr>
      <th rowspan="6" valign="top">1</th>
      <th>count</th>
      <td>483.000000</td>
      <td>483.000000</td>
      <td>483.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>206.914079</td>
      <td>212.410145</td>
      <td>205.231677</td>
    </tr>
    <tr>
      <th>std</th>
      <td>68.997792</td>
      <td>51.728910</td>
      <td>47.132825</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>70.900000</td>
      <td>47.400000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>217.600000</td>
      <td>211.300000</td>
      <td>204.800000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>350.800000</td>
      <td>363.700000</td>
      <td>354.900000</td>
    </tr>
  </tbody>
</table>
</div>
Сделаем то же самое, но немного по-другому, передав в `agg` список функций:


```
columns_to_show = ['Total day minutes', 'Total eve minutes', 'Total night minutes']

df.groupby(['Churn'])[columns_to_show].agg([np.mean, np.std, np.min, np.max])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th colspan="4" halign="left">Total day minutes</th>
      <th colspan="4" halign="left">Total eve minutes</th>
      <th colspan="4" halign="left">Total night minutes</th>
    </tr>
    <tr>
      <th></th>
      <th>mean</th>
      <th>std</th>
      <th>amin</th>
      <th>amax</th>
      <th>mean</th>
      <th>std</th>
      <th>amin</th>
      <th>amax</th>
      <th>mean</th>
      <th>std</th>
      <th>amin</th>
      <th>amax</th>
    </tr>
    <tr>
      <th>Churn</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>175.175754</td>
      <td>50.181655</td>
      <td>0.0</td>
      <td>315.6</td>
      <td>199.043298</td>
      <td>50.292175</td>
      <td>0.0</td>
      <td>361.8</td>
      <td>200.133193</td>
      <td>51.105032</td>
      <td>23.2</td>
      <td>395.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>206.914079</td>
      <td>68.997792</td>
      <td>0.0</td>
      <td>350.8</td>
      <td>212.410145</td>
      <td>51.728910</td>
      <td>70.9</td>
      <td>363.7</td>
      <td>205.231677</td>
      <td>47.132825</td>
      <td>47.4</td>
      <td>354.9</td>
    </tr>
  </tbody>
</table>
</div>

### Сводные таблицы

Допустим, мы хотим посмотреть, как наблюдения в нашей выборке распределены в контексте двух признаков — `Churn` и `International plan`. Для этого мы можем построить **таблицу сопряженности**, воспользовавшись методом **`crosstab`**:


```
pd.crosstab(df['Churn'], df['International plan'])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>International plan</th>
      <th>No</th>
      <th>Yes</th>
    </tr>
    <tr>
      <th>Churn</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2664</td>
      <td>186</td>
    </tr>
    <tr>
      <th>1</th>
      <td>346</td>
      <td>137</td>
    </tr>
  </tbody>
</table>
</div>

```
pd.crosstab(df['Churn'], df['Voice mail plan'], normalize=True)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Voice mail plan</th>
      <th>No</th>
      <th>Yes</th>
    </tr>
    <tr>
      <th>Churn</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.602460</td>
      <td>0.252625</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.120912</td>
      <td>0.024002</td>
    </tr>
  </tbody>
</table>
</div>
Мы видим, что большинство пользователей лояльны и при этом пользуются дополнительными услугами (международного роуминга / голосовой почты).

Продвинутые пользователи Excel наверняка вспомнят о такой фиче, как **сводные таблицы** (pivot tables). В Pandas за сводные таблицы отвечает метод **`pivot_table`**, который принимает в качестве параметров:

* `values` – список переменных, по которым требуется рассчитать нужные статистики,
* `index` – список переменных, по которым нужно сгруппировать данные,
* `aggfunc` — то, что нам, собственно, нужно посчитать по группам — сумму, среднее, максимум, минимум или что-то ещё.

Давайте посмотрим среднее число дневных, вечерних и ночных звонков для разных Area code:


```
df.pivot_table(['Total day calls', 'Total eve calls', 'Total night calls'], ['Area code'], aggfunc='mean').head(10)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Total day calls</th>
      <th>Total eve calls</th>
      <th>Total night calls</th>
    </tr>
    <tr>
      <th>Area code</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>408</th>
      <td>100.496420</td>
      <td>99.788783</td>
      <td>99.039379</td>
    </tr>
    <tr>
      <th>415</th>
      <td>100.576435</td>
      <td>100.503927</td>
      <td>100.398187</td>
    </tr>
    <tr>
      <th>510</th>
      <td>100.097619</td>
      <td>99.671429</td>
      <td>100.601190</td>
    </tr>
  </tbody>
</table>
</div>

### Преобразование датафреймов

Как и многое другое в Pandas, добавление столбцов в DataFrame осуществимо несколькими способами.

Например, мы хотим посчитать общее количество звонков для всех пользователей. Создадим объект `total_calls` типа Series и вставим его в датафрейм:


```
total_calls = df['Total day calls'] + df['Total eve calls'] + df['Total night calls'] + df['Total intl calls']
df.insert(loc=len(df.columns), column='Total calls', value=total_calls) 
# loc - номер столбца, после которого нужно вставить данный Series
# мы указали len(df.columns), чтобы вставить его в самом конце
df.head()
```

<img src="https://habrastorage.org/files/7d9/24f/a03/7d924fa03ed745b691b56dd4c1b7b3ba.png"/>
<br>

Добавить столбец из имеющихся можно и проще, не создавая промежуточных Series:


```
df['Total charge'] = df['Total day charge'] + df['Total eve charge'] + df['Total night charge'] + df['Total intl charge']

df.head()
```

<img src="https://habrastorage.org/files/231/b67/a22/231b67a2294f42a8a82e1587dfe214e2.png"/>
<br>

Чтобы удалить столбцы или строки, воспользуйтесь методом `drop`, передавая в качестве аргумента нужные индексы и требуемое значение параметра `axis` (`1`, если удаляете столбцы, и ничего или `0`, если удаляете строки):

```
df = df.drop(['Total charge', 'Total calls'], axis=1) # избавляемся от созданных только что столбцов

df.drop([1, 2]).head() # а вот так можно удалить строчки
```

<img src="https://habrastorage.org/files/06e/cd4/df4/06ecd4df461a4e20a376d1ecb0138573.png"/>
<br>


## 4. Первые попытки прогнозирования оттока


Посмотрим, как отток связан с признаком *"Подключение международного роуминга" (International plan)*. Сделаем это с помощью сводной таблички *crosstab*, а также путем иллюстрации с Seaborn (как именно строить такие картинки и анализировать с их помощью графики – материал следующей статьи).

```python
pd.crosstab(df['Churn'], df['International plan'], margins=True)
```

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>International plan</th>
      <th>False</th>
      <th>True</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Churn</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2664</td>
      <td>186</td>
      <td>2850</td>
    </tr>
    <tr>
      <th>1</th>
      <td>346</td>
      <td>137</td>
      <td>483</td>
    </tr>
    <tr>
      <th>All</th>
      <td>3010</td>
      <td>323</td>
      <td>3333</td>
    </tr>
  </tbody>
</table>
</div>

<img src="https://habrastorage.org/files/bc3/39d/60b/bc339d60b38f43e682f17ba767e8a4e6.png"/>
<br>

Видим, что когда роуминг подключен, доля оттока намного выше – интересное наблюдение! Возможно, большие и плохо контролируемые траты в роуминге очень конфликтогенны и приводят к недовольству клиентов телеком-оператора и, соответственно, к их оттоку. 

Далее посмотрим на еще один важный признак – *"Число обращений в сервисный центр" (Customer service calls)*. Также построим сводную таблицу и картинку.


```python
pd.crosstab(df['Churn'], df['Customer service calls'], margins=True)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Customer service calls</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Churn</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>605</td>
      <td>1059</td>
      <td>672</td>
      <td>385</td>
      <td>90</td>
      <td>26</td>
      <td>8</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
      <td>2850</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92</td>
      <td>122</td>
      <td>87</td>
      <td>44</td>
      <td>76</td>
      <td>40</td>
      <td>14</td>
      <td>5</td>
      <td>1</td>
      <td>2</td>
      <td>483</td>
    </tr>
    <tr>
      <th>All</th>
      <td>697</td>
      <td>1181</td>
      <td>759</td>
      <td>429</td>
      <td>166</td>
      <td>66</td>
      <td>22</td>
      <td>9</td>
      <td>2</td>
      <td>2</td>
      <td>3333</td>
    </tr>
  </tbody>
</table>
</div>

<img src="https://habrastorage.org/files/fed/e25/781/fede257819424580ba6f16da0dc452e7.png"/>
<br>

Может быть, по сводной табличке это не так хорошо видно (или скучно ползать взглядом по строчкам с цифрами), а вот картинка красноречиво свидетельствует о том, что доля оттока сильно возрастает начиная с 4 звонков в сервисный центр. 

Добавим теперь в наш DataFrame бинарный признак — результат сравнения `Customer service calls > 3`. И еще раз посмотрим, как он связан с оттоком. 


```python
df['Many_service_calls'] = (df['Customer service calls'] > 3).astype('int')

pd.crosstab(df['Many_service_calls'], df['Churn'], margins=True)
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Churn</th>
      <th>0</th>
      <th>1</th>
      <th>All</th>
    </tr>
    <tr>
      <th>Many_service_calls</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2721</td>
      <td>345</td>
      <td>3066</td>
    </tr>
    <tr>
      <th>1</th>
      <td>129</td>
      <td>138</td>
      <td>267</td>
    </tr>
    <tr>
      <th>All</th>
      <td>2850</td>
      <td>483</td>
      <td>3333</td>
    </tr>
  </tbody>
</table>
</div>

<img src="https://habrastorage.org/files/9df/85e/6c9/9df85e6c9bde483ab46f45131718afd7.png"/>
<br>


Объединим рассмотренные выше условия и построим сводную табличку для этого объединения и оттока.


```python
pd.crosstab(df['Many_service_calls'] & df['International plan'] , df['Churn'])
```




<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Churn</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>row_0</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>False</th>
      <td>2841</td>
      <td>464</td>
    </tr>
    <tr>
      <th>True</th>
      <td>9</td>
      <td>19</td>
    </tr>
  </tbody>
</table>
</div>
Значит, прогнозируя лояльность клиента в случае, когда число звонков в сервисный центр меньше 4 и не подключен роуминг (и прогнозируя отток – в противном случае), можно ожидать процент "угадывания лояльности клиента" около 85.8% (ошибаемся всего 464 + 9 раз). Эти 85.8%, которые мы получили с помощью очень простых рассуждений – это неплохая отправная точка (*baseline*) для дальнейших моделей машинного обучения, которые мы будем строить. 

В целом до появления машинного обучения процесс анализа данных выглядел примерно так. Прорезюмируем:
    
- Доля лояльных клиентов в выборке – 85.5%. Самая наивная модель, ответ которой "клиент всегда лоялен" на подобных данных будет угадывать примерно в  85.5% случаев. То есть доли правильных ответов (*accuracy*) последующих моделей должны быть как минимум не меньше, а лучше, значительно выше этой цифры;
- С помощью простого прогноза , который условно можно выразить такой формулой: "International plan = False & Customer Service calls < 4 => Churn = 0, else Churn = 1", можно ожидать долю угадываний 85.8%, что еще чуть выше 85.5%. Впоследствии мы поговорим о деревьях решений и разберемся, как находить подобные правила автоматически на основе только входных данных;
- Эти два бейзлайна мы получили без всякого машинного обучения, и они служат отправной точной для наших последующих моделей. Если окажется, что мы громадными усилиями увеличиваем долю правильных ответов всего, скажем, на 0.5%, то возможно, мы что-то делаем не так, и достаточно ограничиться простой моделью из двух условий;
- Перед обучением сложных моделей рекомендуется немного покрутить данные и проверить простые предположения. Более того, в бизнес-приложениях машинного обучения чаще всего начинают именно с простых решений, а потом экспериментируют с их усложнениями. 


## 5. Домашнее задание №1

Первое домашнее задание посвящено анализу набора данных **Adult** репозитория UCI и представлено в [этом Jupyter-ноутбуке](https://github.com/Yorko/mlcourse_open/blob/master/jupyter_notebooks/topic1_pandas_data_analysis/hw1_adult_pandas.ipynb). В датасете Adult собрана демографическая информация по жителям США. 

Мы предлагаем выполнить это задание и затем ответить на 10 вопросов. [Ссылка](https://goo.gl/forms/pB1tDIArMvTUsIUg2) на форму для ответов (она же – в ноутбуке). Ответы в форме можно менять после отправки, но не после дедлайна (точнее, технически можно и после дедлайна, но это не будет зачтено).

**Дедлайн:** 6 марта 23:59 (жесткий).

## 6. Обзор полезных ресурсов

* В первую очередь, конечно же, [официальная документация Pandas](http://pandas.pydata.org/pandas-docs/stable/index.html). В частности, рекомендуем короткое введение [10 minutes to pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html),
* [PDF-шпаргалка по библиотеке](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf),
* Отличная презентация Александра Дьяконова [«Знакомство с Pandas»](https://alexanderdyakonov.wordpress.com/2015/11/06/знакомство-с-pandas-слайды/),
* На гитхабе есть [подборка упражнений](https://github.com/guipsamora/pandas_exercises/) по Pandas и [еще один](https://github.com/TomAugspurger/effective-pandas) полезный репозиторий (на английском языке),
* [scipy-lectures.org](http://www.scipy-lectures.org/index.html) — учебник по работе с pandas, numpy, matplotlib и scikit-learn.

*Статья написана в соавторстве с @yorko (Юрием Кашницким).*